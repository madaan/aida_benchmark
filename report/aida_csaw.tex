\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{longtable}
\usepackage{tcolorbox}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{capt-of}
\usepackage{url}
%opening
\title{Evaluating AIDA using hand annotated data}
\author{Aman Madaan}

\begin{document}

\maketitle

\begin{abstract}
Aim of this excercise was to find out how AIDA performs with hand annotated data. Label for the data were obtained
with AIDA and ratio of number of annotations made by AIDA to the number of annotations in the labeled data for a particular entity
is used as the metric to evalutate the success. The mean value of 1.12 indicates that AIDA did pretty well for
entities that were recogized by both AIDA and CSAW. However, the recall is quite low, $0.2606, \frac{989}{3795}$.
Low recall is partially explained by a large number of extraneous tags in CSAW data.
\end{abstract}

\section{Setup}
\subsection{Labelled data}
Labeled data was taken from \url{http://www.cse.iitb.ac.in/soumen/doc/CSAW/Annot/}
\begin{itemize}
 \item 104 annotated files
 \item 3795 entities
 \item 19000 spots 
\end{itemize}

The annotations are available in XML format. A sample annotation is as follows : 
\begin{verbatim}
 <annotation>
	<docName>ganeshTestDoc.txt</docName>
	<userId>amitsingh</userId>
	<wikiName>Sachin Tendulkar</wikiName>
	<offset>420</offset>
	<length>9</length>
</annotation>
\end{verbatim}

\subsection{Benchmarking AIDA with CSAW data}
To compare how AIDA performs, we obtained annotations from AIDA in the same XML schema as
provided by CSAW team. The userId given was AIDA.
\begin{verbatim}
 <annotation>
<docName>ganeshTestDoc.txt</docName>
<userId>AIDA</userId>
<wikiName>Mike Denness</wikiName>
<offset>17</offset>
<length>12</length>
</annotation>
\end{verbatim}

\subsection{Annotated files}
XML file containing annotations for 104 files as above and for other 560 files is available at 
\url{www.cse.iitb.ac.in/~amanmadaan/store}

\subsection{Code}
The source code (without jar files) is hosted at \url{http://github.com/madaan/aida_benchmark}
\newpage
\section{Annotation Statistics}
\begin{center}
\bigskip \bigskip \bigskip \bigskip

\begin{tabular}{|l|l|}
 \hline
Total entities annotated by AIDA & 989\\ 
Total entities annotated by CSAW Team & 3795\\ 
Total entities common to both & 483\\ 
CSAW Entities missed by AIDA : (full list follows)  & 3312\\ 
AIDA Entities missed by CSAW : (full list follows)  & 506\\ 
\hline
\end{tabular}
\captionof{table}{Statistics on Entities}
\end{center}

\bigskip 
\bigskip \bigskip 
\begin{center}

\begin{tabular}{|l|l|}
 \hline
$n$ & 483\\ 
$min$ & 0.03333333333333333\\ 
$max$ & 9.0\\ 
$mean$ & 1.121289147465606\\ 
$\text{std dev}$ & 0.9020144035396026\\ 
$median$ & 1.0\\ 
$skewness$ & 4.657584815674553\\ 
$kurtosis$ & 28.2250405390826\\ 
\hline
\end{tabular}
\captionof{table}{Statistics for $score = \frac{\text{Annotations by AIDA}}{\text{Annotations by CSAW}}$}
\end{center}

\newpage
\section{List of entities Annotated by both CSAW and AIDA}
The score is defined as $score = \frac{\text{Annotations by AIDA}}{\text{Annotations by CSAW}}$
\begin{longtable}{l|l|l|l}
\hline
Entity Name & CSAW Count & AIDA Count & Ratio \\
\hline
\input{entitiesInBoth}
\hline
\end{longtable}

\end{document}
% 